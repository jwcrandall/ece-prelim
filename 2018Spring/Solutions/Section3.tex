\documentclass[main.tex]{subfiles}
\begin{document}
\begin{enumerate}

\subsection*{Section 3 Communications \& Networks and Signal \& Image Processing, Systems \& Controls}

\item [7.] A binary source generates a sequence of symbols with probabilities p and 1-p, respectively. Given the first symbol in the sequence, the source continues to generate symbols until the opposite symbol is generated. Let X denote the length of the sequence, including the first symbol.

    \begin{enumerate}
        \item \textbf{Q.} Find the probability mass function of X. \textbf{Theory.} The probability mass function (pmf) (or frequency function) of a discrete random variable $X$ assigns probabilities to the possible values of the random variable. More specifically, if $x_1, x_2, \ldots$ denote the possible values of a random variable $X$, then the probability mass function is denoted as $p$ and we write
        
        $$
        p\left(x_i\right)=P\left(X=x_i\right)=P(\underbrace{\left\{s \in S \mid X(s)=x_i\right\}}_{\text {set of outcomes resulting in } X=x_i}) .
        $$
        
        $p\left(x_i\right)$ is shorthand for $P\left(X=x_i\right)$, which represents the probability of the event that the random variable $X$ equals $x_i$. The geometric distribution gives the probability that the first occurrence of success requires $k$ independent trials, each with success probability $p$. If the probability of success on each trial is $p$, then the probability that the $k$ th trial is the first success is
        
        $$
        \operatorname{Pr}(X=k)=(1-p)^{k-1} p
        $$
        
        for $k=1,2,3,4, \ldots$. The above form of the geometric distribution is used for modeling the number of trials up to and including the first success. \textbf{A.} Suppose the $1^{\text{st}}$ symbol is generated with probability $1-p_o$ and the opposite symbol with probability $p_o$. The probability mass function of the sequence length $X$ such that the opposite symbol occurs at the $k^{\text{th}}$ time is
        
        $$
        P[X = k] = (1-p_o)^{k-1}p_o
        $$
        
        where $p_o>0, k \in 1,2,3,\dots$.
        
        \item \textbf{Q.} Find the expected value of X. \textbf{Theory} The mean $\mu$ (or expected value $E[X]$) of a random variable $X$ is the sum of the weighted possible values for $X$; weighted, that is, by their respective probabilities. If $S$ is the set of all possible values for $X$, then the formula for the mean is:

        $$
        \mu &= \sum_{x \in S} x \cdot p(x)\\
        $$

        Given that $0<1-p<1$ we can use the geometric series formula to obtain:
        
        $$
        \left(\sum_{k=1}^{\infty}(1-p)^k\right)=\frac{1-p}{p}
        $$

        \textbf{A.}
        
        $$
        \begin{aligned}
        E[X] &= \sum_{k=1}^{\infty} k(1-p)^{k-1} \cdot p \\
        & =p(1-p)^{-1} \sum_{k=1}^{\infty} k(1-p)^k \\
        & =\frac{p}{1-p} \sum_{k=1}^{\infty} k q^k, q=1-p \\
        S & =\sum_{k=1}^{\infty} k q^k\\
        &=q+2 q^2+3 q^3+\ldots \\
        qS & =q^2+2 q^3+\ldots \\
        (1-q)S &= q+q^2+q^3+\ldots && \text{geometric series}\\
        &= \frac{q}{1-q} \\
        S &= \frac{q}{(1-q)^2}\\
        E[X] &= \frac{p}{1-p} \times \frac{1-p}{p^2}\\
        &=\frac{1}{p}
        \end{aligned}
        $$ 
 
    \end{enumerate}
    
\item [8.] Messages arriving at a central office switch are exponentially distributed in length, with average length 800 bits and average arrival rate of 16 messages per second. The switch has an infinite buffer and is served by a 64 kilobit per second transmission circuit.

    \begin{enumerate}
        \item \textbf{Q.} Determine the traffic intensity for the switch in Erlangs. \textbf{Theory} The erlang (symbol E) is a dimensionless unit that is used in telephony as a measure of offered load or carried load on service-providing elements such as telephone circuits or telephone switching equipment. In a digital network, the traffic intensity is:
        
        $$
        \frac{a L}{R}
        $$
        
        where $a$ is the average arrival rate of packets (e.g. in packets per second), $L$ is the average packet length (e.g. in bits), and $R$ is the transmission rate (e.g. bits per second). A traffic intensity greater than one erlang means that the rate at which bits arrive exceeds the rate bits can be transmitted and queuing delay will grow without bound (if the traffic intensity stays the same). If the traffic intensity is less than one erlang, then the router can handle more average traffic. \textbf{A.} Length of a message $=800$ bits. Arrival Rate $(\lambda)=16$ $\mathrm{messages}/\mathrm{sec}$. ie $\lambda=800 \times 16$ $\mathrm{bits}/\mathrm{sec}$ and Service Rate $(\mu)=64 \times 10^3$ $\mathrm{bits}/\mathrm{sec}$. 
        
        $$
        \begin{aligned}
        \text { Traffic Intensity } \rho& =\frac{\lambda}{\mu} \\
        & =\frac{800 \times 16}{64 \times 10^3} \\
        & =0.2 \\
        \end{aligned}
        $$

        \item \textbf{Q.} Determine the probability distribution of the number of messages in the buffer. \textbf{Theory} The $\mathrm{M} / \mathrm{M} / 1$ model is characterized by the following assumptions: Jobs arrive according to a Poisson process with parameter $\lambda t$, or equivalently, the time between arrivals, $t$, has an exponential distribution with parameter $\lambda$, i.e., for $t \geq 0$, the probability density function is $f(t)=\lambda e^{-\lambda t}$. The service time, $s$, has an exponential distribution with parameter $\mu$, i.e., for $s \geq 0$, the probability density function is $g(s)=\mu e^{-\mu t}$. There is a single server. The buffer is of infinite size and the number of potential jobs is infinite. Definition. The utilization, $\rho$, is the average arrival rate $\mathrm{x}$ average service time. The distribution of inter-arrival times is exponential, hence the average inter arrival time $\bar{t}=\frac{1}{\lambda}$. The distribution of service times is exponential, hence the average service time, $\bar{s}=\frac{1}{\mu}$ and thus, $\rho=\frac{\lambda}{\mu}$. Assumption $\rho<1$. Without this condition, the queue would grow without limit. Proposition. The probability that there $n$ jobs in the system (either queue or process) is 
        
        $$
        P_n=\rho^n[1-\rho].
        $$ 
        
        In a steady state, the expected number of transitions from $n$ up to $n+1$ must equal the number of transitions from $n+1$ down to $n$, or 
        
        $$
        \lambda P_n=\mu P_{n+1}.
        $$
        
        For $n=0$, we have
        
        $$
        P_1=\frac{\lambda}{\mu} P_0=\rho P_0.
        $$
        
        Similarly, by applying repeatedly, we have
        
        $$
        P_n=\rho^n P_0 \text {. }
        $$
        
        To solve for $P_0$, observe that
        
        $$
        \sum_{n=0}^{\infty} P_n=1
        $$
        
        Hence,
        
        $$
        1=P_0 \sum_{n=0}^{\infty} \rho^n=P_0 \frac{1}{1-\rho},
        $$
        
        since $\rho<1$. So implies that
        
        $$
        P_0=1-\rho
        $$

        The expected number of jobs in the system (either queue or process) is 
        
        $$
        L=\sum_{n=0}^{\infty} n P_n=\sum_{n=0}^{\infty} n \rho^n[1-\rho].
        $$
        
        Simplifying, we have

        $$
        \begin{aligned}
        L= & \rho[1-\rho] \sum_{n=0}^{\infty} \frac{d}{d \rho} \rho^n \\
        & =\rho[1-\rho] \frac{d}{d \rho} \sum_{n=0}^{\infty} \rho^n \\
        & =\rho[1-\rho] \frac{d}{d \rho}\left(\frac{1}{1-\rho}\right)=\frac{\rho}{1-\rho}=\frac{\lambda}{\mu-\lambda}
        \end{aligned}
        $$
        
        Since there is a single server, the expected number of jobs in the queue is
        
        $$
        \begin{aligned}
        L_q= & \sum_{n=1}^{\infty}[n-1] P_n=\sum_{n=1}^{\infty}[n-1] \rho^n[1-\rho] \\
        & =\rho[1-\rho] \sum_{n-1=0}^{\infty}[n-1] \rho^{n-1}=\frac{\rho^2}{1-\rho}=\frac{\lambda^2}{\mu[\mu-\lambda]} .
        \end{aligned}
        $$
        
        Little's Formula. In a steady state, the average time spent waiting in the queue,
        
        $$
        W_q=\frac{L_q}{\lambda}
        $$
        
        and the average time spent in the system (in queue or process),
        
        $$
        W=\frac{L}{\lambda} \text {. }
        $$
        
        (Little's Formula is valid for the steady state of any queueing process.) Applying Little's Formula,
        
        $$
        W=\frac{1}{\mu-\lambda},
        $$
        
        and
        
        $$
        W_q=\frac{\lambda}{\mu[\mu-\lambda]}.
        $$

        \textbf{A.}

        $$
        \begin{aligned}
        L_q & = \frac{\lambda^2}{\mu(\mu-\lambda)} \\
        & = \frac{(12800)^2}{64000(64000-12800)} \\
        & = \frac{12800 \times 12800}{64000 \times 51200} \\
        & = 0.05
        \end{aligned}
        $$
        
        \item \textbf{Q.}Determine the average waiting time of a message in the buffer in seconds. \textbf{Theory} see b \textbf{A.} 
        
        $$
        \begin{aligned}
        W_q & =\frac{\lambda}{\mu(\mu-\lambda)} \\
        & =\frac{12800}{64000(64000-12800)} \\
        & =\frac{1}{64000 \times 51250} \\
        \end{aligned}
        $$
        
        \item \textbf{Q.}Determine the total average time a message spends in the system, including the waiting time and the service time. \textbf{Theory} see b \textbf{A.} 
        
        $$
        \begin{aligned}
        W &= \frac{1}{\mu-\lambda}\\
        &= \frac{1}{64000-12800} \\
        &= \frac{1}{51200}
        \end{aligned}
        $$
        
    \end{enumerate}

\item [9.] Let $\{\mathrm{Xn}: \mathrm{n}=1,2 \ldots\}$ be an infinite sequence of independent binary random variables with sample values \{0,1) and $\mathrm{P}\{\mathrm{Xn}=0\} = 2/3$. Let $\mathrm{Yn}=\sum_{\mathrm{i}=1}^{\mathrm{n}} \mathrm{Xi}$ be a random process defined by $\mathrm{Xn}$.

    \begin{enumerate}
        \item \textbf{Q.} For n=5, determine all sample functions of the random process. \textbf{Theory} The random process $\mathrm{Yn}=\sum_{\mathrm{i}=1}^{\mathrm{n}} \mathrm{Xi}$ is a sum of independent binary random variables $\{\mathrm{Xn}: \mathrm{n}=1,2 \ldots\}$ with sample values $\{0,1)$ and $P\{X n=0\}=2 / 3$. For $n=5$, the sample functions of the random process are all possible values of Yn for all possible combinations of $\mathrm{Xn}$. Yn can take on any value between 0 and 5 . The probability of Yn taking on a particular value $k$ is given by the binomial distribution with parameters $n$ and $p$, where $p$ is the probability of success (i.e., $\mathrm{P}\{\mathrm{Xn}=1\}$ ) and $n$ is the number of trials (i.e., $n=5$ ). Thus, the probability mass function (PMF) of Yn is given by:
        
        $$
        \begin{aligned}
        \mathrm{P}\{\mathrm{Yn}=k\} & =\left(\begin{array}{l}
        n \\
        k
        \end{array}\right) p^k(1-p)^{n-k} \\
        & = \frac{n !}{k !(n-k) !} p^k(1-p)^{n-k}\\ 
        & = \left(\begin{array}{l}
        5 \\
        k
        \end{array}\right)\left(\frac{1}{3}\right)^k\left(\frac{2}{3}\right)^{5-k}
        \end{aligned}
        $$

        \textbf{Recall} $0! = 1$. \textbf{A.} Therefore, the sample functions of the random process for $\mathrm{n}=5$ are:
        
        
        - $\mathrm{Y}5=0$ with probability 

        $$
        \binom{5}{0}\left(\frac{1}{3}\right)^0\left(\frac{2}{3}\right)^{5-0} = \frac{5 !}{0 !(5-0) !}\left(\frac{2}{3}\right)^5=\frac{32}{243}
        $$
        
        - $Y 5=1$ with probability
        
        $$
        5\left(\frac{1}{3}\right)\left(\frac{2}{3}\right)^4=\frac{160}{243}
        $$
        
        - $Y 5=2$ with probability
        
        $$
        10\left(\frac{1}{3}\right)^2\left(\frac{2}{3}\right)^3=\frac{200}{243}
        $$
        
        - $Y 5=3$ with probability
        
        $$
        10\left(\frac{1}{3}\right)^3\left(\frac{2}{3}\right)^2=\frac{100}{243}
        $$
        
        - $\mathrm{Y} 5=4$ with probability
        
        $$
        5\left(\frac{1}{3}\right)^4\left(\frac{2}{3}\right)=\frac{20}{243}
        $$
        
        - $\mathrm{Y} 5=5$ with probability
        
        $$
        \left(\frac{1}{3}\right)^5=\frac{1}{243}
        $$
        
        \item \textbf{Q.} Determine the probability mass function of Yn. \textbf{A.} The probability mass function of Yn can be found by using the following formula:
        
        $$
        P\left(Y_n=k\right)=\sum_{i=k}^n\left(\begin{array}{l}
        n \\
        i
        \end{array}\right)\left(\frac{1}{3}\right)^i\left(\frac{2}{3}\right)^{n-i}
        $$
        
        where $k$ is an integer between 0 and $n$. This formula is derived from the fact that $Y_n$ is a sum of independent Bernoulli random variables with parameter $p=1/3$. The probability that $Y_n$ takes on a particular value $k$ is equal to the sum of the probabilities of all possible combinations of $k$ successes and $(n-k)$ failures in the sequence of $n$ trials.
        
        \item \textbf{Q.} Find the expected value and variance of $\mathrm{Yn}$. \textbf{A.} The expected value of $Y_n$ is given by:
        
        $$
        E\left(Y_n\right)=E\left(\sum_{i=1}^n X_i\right)=\sum_{i=1}^n E\left(X_i\right)=\sum_{i=1}^n \frac{1}{3}=\frac{n}{3}
        $$
        
        The variance of $Y_n$ is given by:
        
        $$
        \operatorname{Var}\left(Y_n\right)=\operatorname{Var}\left(\sum_{i=1}^n X_i\right)=\sum_{i=1}^n \operatorname{Var}\left(X_i\right)=\sum_{i=1}^n \frac{2}{9}=\frac{2 n}{9}
        $$
        
        where we have used the fact that $X_i$ is a Bernoulli random variable with parameter $p=1/3$, so $E\left(X_i\right)=p$ and $\operatorname{Var}\left(X_i\right)=p(1-p)$.
        
        \item \textbf{Q.} Find the autocorrelation function of 
        
        $$
        \mathrm{Yn}, \mathrm{R}\{\mathrm{Y}(\mathrm{n}, \mathrm{n}+\mathrm{k})\}=\mathrm{E}\{\mathrm{Yn} \mathrm{Yn}+\mathrm{k}\}
        $$ 

        \textbf{Theory} In statistics, the autocorrelation of a real or complex random process is the Pearson correlation between values of the process at different times, as a function of the two times or of the time lag. Let $\left\{X_t\right\}$ be a random process, and $t$ be any point in time ( $t$ may be an integer for a discrete-time process or a real number for a continuous-time process). Then $X_t$ is the value (or realization) produced by a given run of the process at time $t$. Suppose that the process has mean $\mu_t$ and variance $\sigma_t^2$ at time $t$, for each $t$. Then the definition of the auto-correlation function between times $t_1$ and $t_2$ is 

        $$
        \mathrm{R}_{X X}\left(t_1, t_2\right)=\mathrm{E}\left[X_{t_1} \bar{X}_{t_2}\right]
        $$

        where $\mathrm{E}$ is the expected value operator and the bar represents complex conjugation. Note that the expectation may not be well defined. Subtracting the mean before multiplication yields the auto-covariance function between times $t_1$ and $t_2$:

        $$
        \mathrm{K}_{X X}\left(t_1, t_2\right)=\mathrm{E}\left[\left(X_{t_1}-\mu_{t_1}\right) \overline{\left(X_{t_2}-\mu_{t_2}\right)}\right]=\mathrm{E}\left[X_{t_1} \bar{X}_{t_2}\right]-\mu_{t_1} \bar{\mu}_{t_2}
        $$

        Note that this expression is not well defined for all time series or processes, because the mean may not exist, or the variance may be zero (for a constant process) or infinite (for processes with distribution lacking well-behaved moments, such as certain types of power law).
        
        \textbf{A.} Let $\left\{X_n\right\}$ be an infinite sequence of independent binary random variables with sample values $\{0,1\}$ and $\mathrm{P}\left\{X_n=0\right\}=2 / 3$ . Let $Y_n=\sum_{i=1}^n X_i$ be a random process defined by $X_n$. The autocorrelation function of $Y_n$ is given by:
        
        $$
        R(k)=\mathrm{E}\left[Y_n Y_{n+k}\right]=\mathrm{E}\left[\left(\sum_{i=1}^n X_i\right)\left(\sum_{j=1}^{n+k} X_j\right)\right]
        $$
        
        Expanding the expression inside the expectation, we get: 

        $$
        \begin{aligned} 
        \mathrm{E}\left[\left(\sum_{i=1}^n X_i\right)\left(\sum_{j=1}^{n+k} X_j\right)\right] 
        &= \mathrm{E}\left[\sum_{i=1}^n \sum_{j=1}^{n+k} X_iX_j\right] \\ 
        &= \sum_{i=1}^n \sum_{j=1}^{n+k} \mathrm{E}[X_iX_j] 
        \end{aligned}
        $$

        Since $X_i$ and $X_j$ are independent for $i \neq j$, we have $\mathrm{E}\left[X_i X_j\right]=\mathrm{E}\left[X_i\right] \mathrm{E}\left[X_j\right]$ for $i \neq j$. Also, since $\mathrm{P}\left\{X_n=0\right\}=2 / 3$, we have $\mathrm{E}\left[X_n\right]=1-2 / 3=1 / 3$. Therefore, for $i \neq j$, we have $\mathrm{E}\left[X_i X_j\right]=(1 / 3)(1 / 3)=1 / 9$. For $i=j$, we have $\mathrm{E}\left[X_i^2\right]=\mathrm{E}\left[X_i\right]$ since $X_i$ is a binary random variable. Therefore, for $i=j$, we have $\mathrm{E}\left[X_{\mathrm{i}}^2\right]=1 / 3$.
        Substituting these values into the above expression, we get: 

        $$
        \begin{aligned} 
        \sum_{i=1}^n \sum_{j=1}^{n+k} \mathrm{E}[X_iX_j] &= \sum_{i=1}^n \left(\sum_{j=1, j\neq i}^{n+k} \frac{1}{9} + \frac{1}{3}\right) \ &= n(n+k-1)\frac{1}{9} + n\frac{1}{3} 
        \end{aligned}
        $$
        
        So, the autocorrelation function for the process $Y_n$ is given by:
        
        $$
        R(k)=n(n+k-1) \frac{1}{9}+n \frac{1}{3}
        $$
        
    \end{enumerate}
    
\end{enumerate}
\end{document}