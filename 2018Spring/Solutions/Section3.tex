\documentclass[main.tex]{subfiles}
\begin{document}
\begin{enumerate}

\subsection*{Section 3 Communications \& Networks and Signal \& Image Processing, Systems \& Controls}

\item [7.] A binary source generates a sequence of symbols with probabilities p and 1-p, respectively. Given the first symbol in the sequence, the source continues to generate symbols until the opposite symbol is generated. Let X denote the length of the sequence, including the first symbol.

    \begin{enumerate}
        \item \textbf{Q.} Find the probability mass function of X. \textbf{Theory.} The probability mass function (pmf) (or frequency function) of a discrete random variable $X$ assigns probabilities to the possible values of the random variable. More specifically, if $x_1, x_2, \ldots$ denote the possible values of a random variable $X$, then the probability mass function is denoted as $p$ and we write
        
        $$
        p\left(x_i\right)=P\left(X=x_i\right)=P(\underbrace{\left\{s \in S \mid X(s)=x_i\right\}}_{\text {set of outcomes resulting in } X=x_i}) .
        $$
        
        $p\left(x_i\right)$ is shorthand for $P\left(X=x_i\right)$, which represents the probability of the event that the random variable $X$ equals $x_i$. The geometric distribution gives the probability that the first occurrence of success requires $k$ independent trials, each with success probability $p$. If the probability of success on each trial is $p$, then the probability that the $k$ th trial is the first success is
        
        $$
        \operatorname{Pr}(X=k)=(1-p)^{k-1} p
        $$
        
        for $k=1,2,3,4, \ldots$. The above form of the geometric distribution is used for modeling the number of trials up to and including the first success. \textbf{A.} Suppose the $1^{\text{st}}$ symbol is generated with probability $1-p_o$ and the opposite symbol with probability $p_o$. The probability mass function of the sequence length $X$ such that the opposite symbol occurs at the $k^{\text{th}}$ time is
        
        $$
        P[X = k] = (1-p_o)^{k-1}p_o
        $$
        
        where $p_o>0, k \in 1,2,3,\dots$.
        
        \item \textbf{Q.} Find the expected value of X. \textbf{Theory} The mean $\mu$ (or expected value $E[X]$) of a random variable $X$ is the sum of the weighted possible values for $X$; weighted, that is, by their respective probabilities. If $S$ is the set of all possible values for $X$, then the formula for the mean is:

        $$
        \mu &= \sum_{x \in S} x \cdot p(x)\\
        $$

        Given that $0<1-p<1$ we can use the geometric series formula to obtain:
        
        $$
        \left(\sum_{k=1}^{\infty}(1-p)^k\right)=\frac{1-p}{p}
        $$

        \textbf{A.}
        
        $$
        \begin{aligned}
        E[X] &= \sum_{k=1}^{\infty} k(1-p)^{k-1} \cdot p \\
        & =p(1-p)^{-1} \sum_{k=1}^{\infty} k(1-p)^k \\
        & =\frac{p}{1-p} \sum_{k=1}^{\infty} k q^k, q=1-p \\
        S & =\sum_{k=1}^{\infty} k q^k\\
        &=q+2 q^2+3 q^3+\ldots \\
        qS & =q^2+2 q^3+\ldots \\
        (1-q)S &= q+q^2+q^3+\ldots && \text{geometric series}\\
        &= \frac{q}{1-q} \\
        S &= \frac{q}{(1-q)^2}\\
        E[X] &= \frac{p}{1-p} \times \frac{1-p}{p^2}\\
        &=\frac{1}{p}
        \end{aligned}
        $$ 
 
    \end{enumerate}
    
\item [8.] Messages arriving at a central office switch are exponentially distributed in length, with average length 800 bits and average arrival rate of 16 messages per second. The switch has an infinite buffer and is served by a 64 kilobit per second transmission circuit.

    \begin{enumerate}
        \item \textbf{Q.} Determine the traffic intensity for the switch in Erlangs. \textbf{Theory} The erlang (symbol E) is a dimensionless unit that is used in telephony as a measure of offered load or carried load on service-providing elements such as telephone circuits or telephone switching equipment. In a digital network, the traffic intensity is:
        
        $$
        \frac{a L}{R}
        $$
        
        where $a$ is the average arrival rate of packets (e.g. in packets per second), $L$ is the average packet length (e.g. in bits), and $R$ is the transmission rate (e.g. bits per second). A traffic intensity greater than one erlang means that the rate at which bits arrive exceeds the rate bits can be transmitted and queuing delay will grow without bound (if the traffic intensity stays the same). If the traffic intensity is less than one erlang, then the router can handle more average traffic. \textbf{A.} Length of a message $=800$ bits. Arrival Rate $(\lambda)=16$ $\mathrm{messages}/\mathrm{sec}$. ie $\lambda=800 \times 16$ $\mathrm{bits}/\mathrm{sec}$ and Service Rate $(\mu)=64 \times 10^3$ $\mathrm{bits}/\mathrm{sec}$. 
        
        $$
        \begin{aligned}
        \text { Traffic Intensity } \rho& =\frac{\lambda}{\mu} \\
        & =\frac{800 \times 16}{64 \times 10^3} \\
        & =0.2 \\
        \end{aligned}
        $$

        \item \textbf{Q.} Determine the probability distribution of the number of messages in the buffer. \textbf{Theory} The $\mathrm{M} / \mathrm{M} / 1$ model is characterized by the following assumptions: Jobs arrive according to a Poisson process with parameter $\lambda t$, or equivalently, the time between arrivals, $t$, has an exponential distribution with parameter $\lambda$, i.e., for $t \geq 0$, the probability density function is $f(t)=\lambda e^{-\lambda t}$. The service time, $s$, has an exponential distribution with parameter $\mu$, i.e., for $s \geq 0$, the probability density function is $g(s)=\mu e^{-\mu t}$. There is a single server. The buffer is of infinite size and the number of potential jobs is infinite. Definition. The utilization, $\rho$, is the average arrival rate $\mathrm{x}$ average service time. The distribution of inter-arrival times is exponential, hence the average inter arrival time $\bar{t}=\frac{1}{\lambda}$. The distribution of service times is exponential, hence the average service time, $\bar{s}=\frac{1}{\mu}$ and thus, $\rho=\frac{\lambda}{\mu}$. Assumption $\rho<1$. Without this condition, the queue would grow without limit. Proposition. The probability that there $n$ jobs in the system (either queue or process) is 
        
        $$
        P_n=\rho^n[1-\rho].
        $$ 
        
        In a steady state, the expected number of transitions from $n$ up to $n+1$ must equal the number of transitions from $n+1$ down to $n$, or 
        
        $$
        \lambda P_n=\mu P_{n+1}.
        $$
        
        For $n=0$, we have
        
        $$
        P_1=\frac{\lambda}{\mu} P_0=\rho P_0.
        $$
        
        Similarly, by applying repeatedly, we have
        
        $$
        P_n=\rho^n P_0 \text {. }
        $$
        
        To solve for $P_0$, observe that
        
        $$
        \sum_{n=0}^{\infty} P_n=1
        $$
        
        Hence,
        
        $$
        1=P_0 \sum_{n=0}^{\infty} \rho^n=P_0 \frac{1}{1-\rho},
        $$
        
        since $\rho<1$. So implies that
        
        $$
        P_0=1-\rho
        $$

        The expected number of jobs in the system (either queue or process) is 
        
        $$
        L=\sum_{n=0}^{\infty} n P_n=\sum_{n=0}^{\infty} n \rho^n[1-\rho].
        $$
        
        Simplifying, we have

        $$
        \begin{aligned}
        L= & \rho[1-\rho] \sum_{n=0}^{\infty} \frac{d}{d \rho} \rho^n \\
        & =\rho[1-\rho] \frac{d}{d \rho} \sum_{n=0}^{\infty} \rho^n \\
        & =\rho[1-\rho] \frac{d}{d \rho}\left(\frac{1}{1-\rho}\right)=\frac{\rho}{1-\rho}=\frac{\lambda}{\mu-\lambda}
        \end{aligned}
        $$
        
        Since there is a single server, the expected number of jobs in the queue is
        
        $$
        \begin{aligned}
        L_q= & \sum_{n=1}^{\infty}[n-1] P_n=\sum_{n=1}^{\infty}[n-1] \rho^n[1-\rho] \\
        & =\rho[1-\rho] \sum_{n-1=0}^{\infty}[n-1] \rho^{n-1}=\frac{\rho^2}{1-\rho}=\frac{\lambda^2}{\mu[\mu-\lambda]} .
        \end{aligned}
        $$
        
        Little's Formula. In a steady state, the average time spent waiting in the queue,
        
        $$
        W_q=\frac{L_q}{\lambda}
        $$
        
        and the average time spent in the system (in queue or process),
        
        $$
        W=\frac{L}{\lambda} \text {. }
        $$
        
        (Little's Formula is valid for the steady state of any queueing process.) Applying Little's Formula,
        
        $$
        W=\frac{1}{\mu-\lambda},
        $$
        
        and
        
        $$
        W_q=\frac{\lambda}{\mu[\mu-\lambda]}.
        $$

        \textbf{A.}

        $$
        \begin{aligned}
        L_q & = \frac{\lambda^2}{\mu(\mu-\lambda)} \\
        & = \frac{(12800)^2}{64000(64000-12800)} \\
        & = \frac{12800 \times 12800}{64000 \times 51200} \\
        & = 0.05
        \end{aligned}
        $$
        
        \item \textbf{Q.}Determine the average waiting time of a message in the buffer in seconds. \textbf{Theory} see b \textbf{A.} 
        
        $$
        \begin{aligned}
        W_q & =\frac{\lambda}{\mu(\mu-\lambda)} \\
        & =\frac{12800}{64000(64000-12800)} \\
        & =\frac{1}{64000 \times 51250} \\
        \end{aligned}
        $$
        
        \item \textbf{Q.}Determine the total average time a message spends in the system, including the waiting time and the service time. \textbf{Theory} see b \textbf{A.} 
        
        $$
        \begin{aligned}
        W &= \frac{1}{\mu-\lambda}\\
        &= \frac{1}{64000-12800} \\
        &= \frac{1}{51200}
        \end{aligned}
        $$
        
    \end{enumerate}

\item [9.] Let $\{\mathrm{Xn}: \mathrm{n}=1,2 \ldots\}$ be an infinite sequence of independent binary random variables with sample values \{0,1) and $\mathrm{P}\{\mathrm{Xn}=0\} = 2/3$. Let $\mathrm{Yn}=\sum_{\text{i=1}}^{\text{n}} \mathrm{Xi}$ be a random process defined by $\mathrm{Xn}$.

    \begin{enumerate}
        \item \textbf{Q.} For n=5, determine all sample functions of the random process. \textbf{Theory} The random process $\mathrm{Yn}=\sum_{\mathrm{i}=1}^{\mathrm{n}} \mathrm{Xi}$ is a sum of independent binary random variables $\{\mathrm{Xn}: \mathrm{n}=1,2 \ldots\}$ with sample values $\{0,1)$ and $P\{X n=0\}=2 / 3$. For $n=5$, the sample functions of the random process are all possible values of Yn for all possible combinations of $\mathrm{Xn}$. Yn can take on any value between 0 and 5 . The probability of Yn taking on a particular value $k$ is given by the binomial distribution with parameters $n$ and $p$, where $p$ is the probability of success (i.e., $\mathrm{P}\{\mathrm{Xn}=1\}$ ) and $n$ is the number of trials (i.e., $n=5$ ). Thus, the probability mass function (PMF) of Yn is given by:
        
        $$
        \begin{aligned}
        \mathrm{P}\{\mathrm{Yn}=k\} & =\left(\begin{array}{l}
        n \\
        k
        \end{array}\right) p^k(1-p)^{n-k} \\
        & = \frac{n !}{k !(n-k) !} p^k(1-p)^{n-k}\\ 
        & = \left(\begin{array}{l}
        5 \\
        k
        \end{array}\right)\left(\frac{1}{3}\right)^k\left(\frac{2}{3}\right)^{5-k}
        \end{aligned}
        $$

        \textbf{Recall} $0! = 1$
        
        \textbf{A.} Therefore, the sample functions of the random process for $\mathrm{n}=5$ are:
        
        
        - $\mathrm{Y}5=0$ with probability 

        $$
        \binom{5}{0}\left(\frac{1}{3}\right)^0\left(\frac{2}{3}\right)^{5-0} = \frac{5 !}{0 !(5-0) !}\left(\frac{2}{3}\right)^5=\frac{32}{243}
        $$
        
        - $Y 5=1$ with probability
        
        $$
        5\left(\frac{1}{3}\right)\left(\frac{2}{3}\right)^4=\frac{160}{243}
        $$
        
        - $Y 5=2$ with probability
        
        $$
        10\left(\frac{1}{3}\right)^2\left(\frac{2}{3}\right)^3=\frac{200}{243}
        $$
        
        - $Y 5=3$ with probability
        
        $$
        10\left(\frac{1}{3}\right)^3\left(\frac{2}{3}\right)^2=\frac{100}{243}
        $$
        
        - $\mathrm{Y} 5=4$ with probability
        
        $$
        5\left(\frac{1}{3}\right)^4\left(\frac{2}{3}\right)=\frac{20}{243}
        $$
        
        - $\mathrm{Y} 5=5$ with probability
        
        $$
        \left(\frac{1}{3}\right)^5=\frac{1}{243}
        $$
        
        \item \textbf{Q.} Determine the probability mass function of Yn.\textbf{Theory} The probability mass function of $\mathrm{Yn}$ is the probability of each of the six possible arrangements. Since each random variable has a probability of $2/3$ of being 0 and a probability of $1/3$ of being 1, the probability of each arrangement is $2/3^{\wedge} 5 = 1/6$. Therefore,
        
        $$
        \begin{aligned}
        \mathrm{P}(\mathrm{Yn}=0) & = 1/6 \\
        \mathrm{P}(\mathrm{Yn}=1) & = 1/3 \\
        \mathrm{P}(\mathrm{Yn}=2) & = 1/2
        \end{aligned}
        $$
        
        \textbf {A.} The expected value of $\mathrm{Yn}$ is the sum of the probabilities of each arrangement, multiplied by the value of $\mathrm{Yn}$ for that arrangement.
        
        $$
        \begin{aligned}
        \mathrm{E}(\mathrm{Yn}) & = (1/6)(0) + (1/3)(1) + (1/2)(2) \\
        & = 1/2
        \end{aligned}
        $$  
        
        \item \textbf{Q.} Find the expected value and variance of $\mathrm{Yn}$. \textbf{Theory} The variance of $\mathrm{Yn}$ is the sum of the squared probabilities of each arrangement, multiplied by the squared difference between the value of $\mathrm{Yn}$ for that arrangement and the expected value of $\mathrm{Yn}$. \textbf{A.}
         
        $$
        \begin{aligned}
        \operatorname{Var}(\mathrm{Yn}) &= (1/6)(0-1/2)^{\wedge} 2 + (1/3)(1- 1/2)^{\wedge} 2 + (1 / 2)(2-1 / 2)^{\wedge}2 \\ 
        & = 1/12
        \end{aligned}
        $$
        
        \item \textbf{Q.} Find the autocorrelation function of Yn, $\mathrm{R}\{\mathrm{Y}(\mathrm{n}, \mathrm{n}+\mathrm{k})\}=\mathrm{E}\{\mathrm{Yn} \mathrm{Yn}+\mathrm{k}\}$ \textbf{Theory} The autocorrelation function of $\mathrm{Yn}$ is the expected value of the product of $\mathrm{Yn}$ and $\mathrm{Yn+k}$. Since $\mathrm{Yn}$ is a function of five binary random variables, it is also a function of five independent random variables. Therefore, the autocorrelation function of $\mathrm{Yn}$ is the same as the autocorrelation function of a five dimensional random vector. \textbf{A.} The autocorrelation function of a five-dimensional random vector is $1 / 2$.
        
    \end{enumerate}
    
\end{enumerate}
\end{document}